# ğŸ” Spark SQL & Query Optimization

---

## ğŸ“Œ What is Spark SQL?

> **Spark SQL** is a module for structured data processing using SQL queries, DataFrames, and Datasets. It provides a distributed SQL engine that supports ANSI SQL syntax.

### ğŸ’¡ Why Use It?
- Express complex logic using familiar **SQL**
- Optimized query execution with **Catalyst** & **Tungsten**
- Unified access to data from multiple sources (Hive, Parquet, JDBC, Kafka, etc.)

---

## ğŸ§± Spark SQL Architecture

```
         ğŸ“¥ Data Sources (CSV, Parquet, Hive, JDBC, Kafka)
                          â†“
               ğŸ”„ Spark Catalyst Optimizer
                          â†“
             Logical Plan â†’ Optimized Plan â†’ Physical Plan
                          â†“
                 ğŸ”§ Tungsten Execution Engine
                          â†“
                    ğŸ§ª Output (DataFrame/Result)
```

---

## ğŸ§° Common Syntax

### ğŸ“„ Read Data as DataFrame

```python
# CSV
df = spark.read.csv("file.csv", header=True, inferSchema=True)

# Parquet
df = spark.read.parquet("file.parquet")

# Hive Table
df = spark.sql("SELECT * FROM my_table")
```

### ğŸ§ª Run SQL Queries

```python
df.createOrReplaceTempView("people")

result = spark.sql("""
    SELECT name, AVG(age) as avg_age
    FROM people
    WHERE city = 'Delhi'
    GROUP BY name
""")

result.show()
```

---

## ğŸ“˜ Spark SQL Functions

| Category     | Functions                               |
|--------------|------------------------------------------|
| **String**   | `concat`, `substr`, `lower`, `upper`     |
| **Date/Time**| `current_date()`, `datediff()`, `month()`|
| **Agg**      | `count`, `avg`, `sum`, `min`, `max`      |
| **Window**   | `row_number()`, `rank()`, `dense_rank()` |

---

## âš¡ Performance Optimization Techniques

| Tip                                 | Benefit                                              |
|-------------------------------------|------------------------------------------------------|
| âœ… **Pushdown Filters**             | Reduces data read from disk                          |
| âœ… **Predicate Pushdown** (Parquet) | Filters early, less I/O                              |
| ğŸ§  **Broadcast Join**               | Avoids expensive shuffle when one table is small     |
| ğŸ“¦ **Caching**                      | Stores intermediate data in memory                   |
| ğŸ§¼ **Partition Pruning**            | Skips reading unneeded partitions                    |
| ğŸ§ª **Use Parquet/ORC**              | Columnar formats, faster for Spark                   |
| â›“ï¸ **Avoid UDFs**                  | Harder to optimize, use built-in functions instead   |
| ğŸ¯ **Repartition smartly**          | Avoid skewed shuffles and too many small files       |
| ğŸ” **Use Explain Plan**             | Analyze query plan with `.explain(True)`             |

---

## ğŸ§  Catalyst Optimizer (Spark's Brain)

> Catalyst is Sparkâ€™s extensible **query optimization engine**.

### Steps:
1. **Parse SQL** â†’ Logical Plan
2. **Analyze** â†’ Resolve attributes/types
3. **Optimize** â†’ Constant folding, predicate pushdown, etc.
4. **Physical Plan** â†’ Code generation

---

## ğŸ” EXPLAIN Query Plan

```python
result.explain(True)
```

> Shows the full query execution plan â€” helps you understand how Spark executes your logic.

---

## ğŸ’¾ Caching & Persistence

```python
df.cache()        # Stores in memory
df.persist()      # Use MEMORY_AND_DISK, etc.
df.unpersist()
```

> Great for iterative algorithms, reused DataFrames.

---

## ğŸ§  Broadcast Join

```python
from pyspark.sql.functions import broadcast

# Small table should be broadcasted
result = df1.join(broadcast(df2), "id")
```

âœ… Use when one table is small â†’ avoids shuffle â†’ much faster

---

## ğŸ”„ Partitioning Tips

```python
# Partitioned read
df = spark.read.option("basePath", "/data") \
     .parquet("/data/year=2024/month=05/")

# Repartition before large join
df = df.repartition("id")
```

---

## ğŸ§ª Window Functions Example

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

w = Window.partitionBy("dept").orderBy("salary")

df.withColumn("rank", row_number().over(w)).show()
```

---

## ğŸ“Š Real-world SQL Flow with Spark

```
1ï¸âƒ£ Source: Hive / Kafka / Parquet
           â†“
2ï¸âƒ£ Read into DataFrame (spark.read)
           â†“
3ï¸âƒ£ Transform via SQL (views + spark.sql)
           â†“
4ï¸âƒ£ Optimize (broadcast, cache, explain)
           â†“
5ï¸âƒ£ Sink: Save to DW / Output to Dashboard
```

---

## ğŸ”§ Useful Configurations

```python
spark.conf.set("spark.sql.shuffle.partitions", 200)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10*1024*1024)  # 10MB
```

---

## ğŸš€ Best Practices

| Best Practice                     | Why                                                   |
|----------------------------------|--------------------------------------------------------|
| Use `.explain(True)`             | Debug performance                                      |
| Reuse SparkSession               | Avoid overhead                                         |
| Avoid shuffling unnecessarily    | Reduces time/memory usage                              |
| Always cache reused DFs          | Speeds up iterative queries                            |
| Avoid complex UDFs               | Catalyst can't optimize them                           |

---

## âœ… Summary

- Spark SQL = Powerful SQL engine + Catalyst Optimizer
- Write SQL-like logic on distributed DataFrames
- Combine with Hive, Kafka, JDBC, Parquet, etc.
- Optimize with broadcast, caching, repartition, explain plans


