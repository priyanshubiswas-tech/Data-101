# ğŸš€ End-to-End Project: Spark SQL + DBT for Data Transformation

## ğŸ§  Use Case: Sales Analytics

We have a **raw orders dataset**, and we want to:
- Clean and transform the data
- Create monthly revenue reports per customer
- Build an incremental pipeline for efficiency

---

## ğŸ“‚ Project Directory Structure

```
dbt-spark-sales/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ stg_orders.sql
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â””â”€â”€ customer_monthly_sales.sql
â”œâ”€â”€ seeds/
â”‚   â””â”€â”€ customers.csv
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ profiles.yml
â””â”€â”€ README.md
```

---

## ğŸ—ï¸ Step-by-Step Pipeline Overview

```
ğŸ“ Raw Data (Parquet/CSV/Hive)
     â¬‡ï¸
ğŸ§¼ Stage: Clean data (dbt model)
     â¬‡ï¸
ğŸ“Š Mart: Aggregate customer monthly sales
     â¬‡ï¸
âœ… Test: Validate the output
     â¬‡ï¸
ğŸ“¦ Materialized as Tables in Spark
```

---

## 1ï¸âƒ£ Project Setup

### ğŸ“„ `profiles.yml`

```yaml
# ~/.dbt/profiles.yml
spark:
  target: dev
  outputs:
    dev:
      type: spark
      method: thrift
      host: localhost
      port: 10001
      schema: analytics
      threads: 2
```

### ğŸ“„ `dbt_project.yml`

```yaml
name: 'dbt_spark_sales'
version: '1.0'
profile: 'spark'

model-paths: ["models"]
seed-paths: ["seeds"]

models:
  dbt_spark_sales:
    staging:
      materialized: view
    marts:
      materialized: table
```

---

## 2ï¸âƒ£ Stage Model: Clean Orders Data

ğŸ“„ `models/staging/stg_orders.sql`

```sql
WITH renamed AS (
  SELECT
    order_id,
    customer_id,
    CAST(order_date AS DATE) AS order_date,
    amount
  FROM {{ source('raw', 'orders') }}
)
SELECT *
FROM renamed
WHERE order_date IS NOT NULL
```

---

## 3ï¸âƒ£ Mart Model: Monthly Revenue per Customer

ğŸ“„ `models/marts/customer_monthly_sales.sql`

```sql
{{ config(materialized='incremental', unique_key='customer_month') }}

WITH monthly_sales AS (
  SELECT
    customer_id,
    DATE_TRUNC('month', order_date) AS month,
    SUM(amount) AS total_sales
  FROM {{ ref('stg_orders') }}
  {% if is_incremental() %}
    WHERE order_date > (SELECT MAX(month) FROM {{ this }})
  {% endif %}
  GROUP BY customer_id, month
)
SELECT
  CONCAT(customer_id, '-', month) AS customer_month,
  customer_id,
  month,
  total_sales
FROM monthly_sales
```

---

## 4ï¸âƒ£ Seeds: Static Data Load

ğŸ“„ `seeds/customers.csv`

```csv
customer_id,customer_name
1,Alice
2,Bob
3,Charlie
```

```bash
dbt seed  # Loads static CSV into Spark
```

---

## 5ï¸âƒ£ Sources: Link Raw Data

ğŸ“„ `models/schema.yml`

```yaml
version: 2

sources:
  - name: raw
    tables:
      - name: orders

models:
  - name: stg_orders
    description: "Cleaned order data"
    columns:
      - name: order_id
        tests:
          - not_null
          - unique
  - name: customer_monthly_sales
    description: "Monthly sales per customer"
    columns:
      - name: total_sales
        tests:
          - not_null
```

---

## 6ï¸âƒ£ Run the Pipeline

```bash
dbt run           # Compiles and runs transformations on Spark SQL
dbt test          # Runs data quality checks
dbt docs generate # Generates documentation site
dbt docs serve    # Serves interactive docs locally
```

---

## âœ… Expected Output

ğŸ“„ Table: `analytics.customer_monthly_sales`

| customer_id | month     | total_sales |
|-------------|-----------|-------------|
| 1           | 2024-01-01| 2300.00      |
| 2           | 2024-01-01| 1800.00      |
| 3           | 2024-02-01| 1500.00      |

---

## ğŸ§  Explanation

### ğŸ”„ Why Incremental?

- Large datasets canâ€™t be recomputed each time.
- `is_incremental()` ensures only new data is processed.

### ğŸ¯ Why DBT with Spark?

| Feature           | Benefit                               |
|------------------|----------------------------------------|
| Modularity        | Break SQL into reusable components     |
| Version Control   | Track transformations via Git          |
| Testing           | Assure quality with built-in tests     |
| Scheduling        | Works with Airflow/cron/dbt Cloud      |
| Scalability       | Leverages Sparkâ€™s distributed power    |

---

## ğŸ§° Bonus Tips

- Use `{{ this }}` â†’ refers to current model in Spark
- Use Jinja logic to write dynamic SQL
- Schedule runs using Airflow or Dagster
- Materialize as `table`, `view`, or `incremental`


