# ğŸ¤– Apache Spark MLlib (Machine Learning Library)

---

## ğŸ“Œ What is MLlib?

> **MLlib** is Apache Sparkâ€™s scalable machine learning library for performing ML on distributed data using **DataFrames** and **Pipelines**.

### âœ… Why Use MLlib?

- Scalable: Works on distributed clusters
- High-level APIs in **Python, Java, Scala**
- Seamlessly integrated with Spark SQL & DataFrames
- Ready-to-use ML algorithms: Classification, Regression, Clustering, Recommendation

---

## ğŸ§± MLlib Architecture

```
        ğŸ”„ Input Data (Structured DataFrame)
                     â†“
       ğŸ” Feature Extraction / Transformation
                     â†“
        ğŸ—ï¸ Pipeline (Stages of processing)
                     â†“
           ğŸ“Š Model Training & Evaluation
                     â†“
               âœ… Predictions
```

---

## ğŸ§  MLlib Components

| Category           | Examples                                              |
|--------------------|-------------------------------------------------------|
| **Algorithms**      | Logistic Regression, Decision Trees, KMeans           |
| **Feature Transformers** | StringIndexer, VectorAssembler, MinMaxScaler   |
| **Pipelines**       | Combine stages into repeatable flows                 |
| **Evaluators**      | Accuracy, RMSE, F1 Score                              |
| **Persistence**     | Save/load models and pipelines                       |

---

## âš™ï¸ Core Steps in MLlib

```
1ï¸âƒ£ Load Data
2ï¸âƒ£ Clean & Transform Features
3ï¸âƒ£ Split into Train/Test
4ï¸âƒ£ Build a Pipeline
5ï¸âƒ£ Fit the Model
6ï¸âƒ£ Evaluate & Predict
```

---

## ğŸ’» PySpark MLlib Classification Example

### ğŸ§ª Logistic Regression on a Titanic-style Dataset

```python
from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import BinaryClassificationEvaluator

spark = SparkSession.builder.appName("MLExample").getOrCreate()

# Load data
df = spark.read.csv("titanic.csv", header=True, inferSchema=True)

# Feature Engineering
indexer = StringIndexer(inputCol="Sex", outputCol="SexIndex")
assembler = VectorAssembler(inputCols=["Pclass", "Age", "SexIndex"], outputCol="features")

# Model
lr = LogisticRegression(featuresCol="features", labelCol="Survived")

# Pipeline
pipeline = Pipeline(stages=[indexer, assembler, lr])

# Split
train, test = df.randomSplit([0.7, 0.3], seed=42)

# Train
model = pipeline.fit(train)

# Predict
predictions = model.transform(test)

# Evaluate
evaluator = BinaryClassificationEvaluator(labelCol="Survived")
print("AUC:", evaluator.evaluate(predictions))
```

---

## ğŸ”„ Common Transformers

| Transformer         | Purpose                          |
|---------------------|----------------------------------|
| `StringIndexer`     | Categorical â†’ Numeric             |
| `OneHotEncoder`     | One-hot encode categorical       |
| `VectorAssembler`   | Combine features into vector     |
| `StandardScaler`    | Normalize features               |
| `MinMaxScaler`      | Scale between 0 and 1            |

---

## ğŸ§  Algorithms in MLlib

| Type         | Algorithm Examples                        |
|--------------|-------------------------------------------|
| Classification | Logistic Regression, Random Forest      |
| Regression     | Linear Regression, GBTRegressor         |
| Clustering     | KMeans                                   |
| Recommendation | ALS (Matrix Factorization)              |
| NLP             | Tokenizer, StopWordsRemover             |

---

## ğŸ“ˆ Evaluation Metrics

| Task           | Evaluator Class                       | Metric Name       |
|----------------|----------------------------------------|-------------------|
| Classification | `BinaryClassificationEvaluator`        | "areaUnderROC"    |
| Regression     | `RegressionEvaluator`                  | "rmse", "r2"      |
| Clustering     | Manual Silhouette Score (custom)       | N/A               |

---

## ğŸ§ª Linear Regression Example

```python
from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol="features", labelCol="price")
model = lr.fit(train_data)
predictions = model.transform(test_data)
```

---

## ğŸ§± Pipeline Structure

```
[Raw Data] 
   â†“
[StringIndexer] â†’ [VectorAssembler] â†’ [Model Estimator]
   â†“
[Trained Model + Predictions]
```

### Sample Pipeline Build

```python
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[indexer, assembler, lr])
model = pipeline.fit(train)
```

---

## ğŸ’¾ Save and Load Models

```python
# Save
model.write().overwrite().save("logistic_model")

# Load
from pyspark.ml.pipeline import PipelineModel
model = PipelineModel.load("logistic_model")
```

---

## ğŸš€ Real-world MLlib Flow Example

```
ğŸ§¾ Input: Customer Data (CSV / Hive / Parquet)
     â¬‡ï¸
ğŸ§¹ Clean & Transform (StringIndexer, VectorAssembler)
     â¬‡ï¸
ğŸ¤– Train Model (e.g., Random Forest)
     â¬‡ï¸
ğŸ“ˆ Evaluate (ROC, Accuracy)
     â¬‡ï¸
ğŸ“¤ Save Model or Predict New Data
```

---

## âœ… Summary

- **Spark MLlib** = Scalable ML for Big Data
- Use **DataFrames + Pipeline API**
- Supports many common algorithms
- Optimized for distributed training
- Save & reuse models in production


