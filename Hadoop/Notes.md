# ğŸ§  Hadoop in Big Data â€” Complete Notes

---

## ğŸ“Œ What is Hadoop?

> Apache Hadoop is an open-source **distributed computing framework** for storing and processing large datasets (Big Data) across clusters of computers using simple programming models.

- Handles **structured, semi-structured, and unstructured data**
- Designed to **scale horizontally**
- Fault-tolerant and cost-effective

---

## ğŸ—ï¸ Hadoop High-Level Architecture (Flow)

```
                ğŸ§‘â€ğŸ’» Client Applications
                         â¬‡ï¸
     +------------------------------------------+
     |           Hadoop Distributed File System |
     |        â¤ NameNode (Master)               |
     |        â¤ DataNodes (Slaves)              |
     +------------------------------------------+
                         â¬‡ï¸
     +------------------------------------------+
     |                YARN (Resource Mgmt)      |
     | â¤ ResourceManager  â¤ NodeManager         |
     +------------------------------------------+
                         â¬‡ï¸
     +------------------------------------------+
     |           MapReduce / Spark Engine       |
     +------------------------------------------+
```

---

## ğŸ—ƒï¸ HDFS (Hadoop Distributed File System)

### ğŸ“¦ What is HDFS?

HDFS is the **storage layer** of Hadoop, used to store large datasets reliably.

| Component  | Role |
|------------|------|
| **NameNode** | Stores metadata (file names, block locations) |
| **DataNode** | Stores actual data blocks                     |

### ğŸ§© Key Concepts:

- Files are split into **blocks** (default 128 MB or 256 MB)
- Each block is **replicated (default 3x)** for fault-tolerance
- NameNode manages filesystem namespace; DataNodes store blocks

### ğŸ”§ HDFS Basic Commands

```bash
# Create directory
hdfs dfs -mkdir /mydir

# Upload file to HDFS
hdfs dfs -put myfile.txt /mydir

# Read a file
hdfs dfs -cat /mydir/myfile.txt

# List files
hdfs dfs -ls /mydir

# Remove file
hdfs dfs -rm /mydir/myfile.txt
```

---

## ğŸ§  MapReduce

MapReduce is Hadoopâ€™s **processing engine** for batch data.

### âš™ï¸ MapReduce Flow

```
    Input Data (HDFS)
           â¬‡ï¸
         Mapper
     (Key-Value Pairs)
           â¬‡ï¸
     Shuffle & Sort
           â¬‡ï¸
         Reducer
    (Aggregated Results)
           â¬‡ï¸
     Output to HDFS
```

### ğŸ§ª Example: Word Count (Pseudo Java Code)

```java
// Mapper
public void map(LongWritable key, Text value, Context context) {
    for (String word : value.toString().split(" ")) {
        context.write(new Text(word), new IntWritable(1));
    }
}

// Reducer
public void reduce(Text key, Iterable<IntWritable> values, Context context) {
    int sum = 0;
    for (IntWritable val : values) {
        sum += val.get();
    }
    context.write(key, new IntWritable(sum));
}
```

---

## ğŸ”— How to Begin a Hadoop Application (Java)

### 1ï¸âƒ£ Setup Hadoop Environment
- Install Hadoop
- Configure `core-site.xml`, `hdfs-site.xml`, `mapred-site.xml`, `yarn-site.xml`

### 2ï¸âƒ£ Compile Java Code
```bash
# Compile Java class
javac -classpath `hadoop classpath` -d wordcount_classes WordCount.java
```

### 3ï¸âƒ£ Create JAR
```bash
jar -cvf wordcount.jar -C wordcount_classes/ .
```

### 4ï¸âƒ£ Run Hadoop Job
```bash
hadoop jar wordcount.jar org.example.WordCount /input /output
```

### 5ï¸âƒ£ View Results
```bash
hdfs dfs -cat /output/part-r-00000
```

---

## âš”ï¸ Hadoop vs Apache Spark

| Feature         | Hadoop (MapReduce)           | Apache Spark                     |
|-----------------|-------------------------------|----------------------------------|
| Processing      | Disk-based batch processing   | In-memory (faster)               |
| Languages       | Java                          | Scala, Python, Java, R           |
| Speed           | Slower                        | 10xâ€“100x faster                  |
| Use Cases       | Batch jobs                    | Batch + Streaming + ML + Graph   |
| Ease of Use     | Verbose (Java code)           | Easy APIs                        |

---

## ğŸ”¥ Hadoop Ecosystem Overview

| Tool     | Function                             |
|----------|--------------------------------------|
| Hive     | SQL-like queries on HDFS             |
| Pig      | Data flow scripts (scripting layer)  |
| HBase    | NoSQL database on HDFS               |
| Sqoop    | RDBMS â†”ï¸ Hadoop data transfer         |
| Flume    | Log ingestion into HDFS              |
| Oozie    | Workflow scheduler                   |
| Zookeeper| Coordination service                 |

---

## ğŸš€ Use Cases of Hadoop

âœ… Clickstream analysis  
âœ… Log processing  
âœ… ETL pipelines  
âœ… Machine learning (w/ Mahout)  
âœ… Healthcare, genomics, finance

---

## ğŸ“ Quick Summary

| âœ… Strengths                 | âš ï¸ Limitations                  |
|-----------------------------|---------------------------------|
| Scalable (horizontal)       | Slower than Spark               |
| Fault-tolerant (HDFS)       | Complex setup/config            |
| Mature open-source tools    | Not ideal for real-time needs   |

---

